{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rwxGnsA92emp"
   },
   "source": [
    "##### Copyright 2018 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "CPII1rGR2rF9"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JtEZ1pCPn--z"
   },
   "source": [
    "# Custom training: walkthrough\n",
    "# カスタム訓練：ウォークスルー"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GV1F7tVTN3Dn"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/beta/tutorials/eager/custom_training_walkthrough\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/ja/beta/tutorials/eager/custom_training_walkthrough.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/ja/beta/tutorials/eager/custom_training_walkthrough.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/ja/beta/tutorials/eager/custom_training_walkthrough.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LDrzLFXE8T1l"
   },
   "source": [
    "This guide uses machine learning to *categorize* Iris flowers by species. It uses TensorFlow to:\n",
    "\n",
    "このガイドでは機械学習を使ってアヤメの花を種によって*分類*します。TensorFlowを使って下記のことを行います。\n",
    "\n",
    "1. Build a model,\n",
    "2. Train this model on example data, and\n",
    "3. Use the model to make predictions about unknown data.\n",
    "\n",
    "1. モデルを構築し、\n",
    "2. このモデルをサンプルデータを使って訓練し、\n",
    "3. このモデルを使って未知のデータに対する予測を行う\n",
    "\n",
    "## TensorFlow programming\n",
    "## TensorFlow プログラミング\n",
    "\n",
    "This guide uses these high-level TensorFlow concepts:\n",
    "\n",
    "このガイドでは、次のような TensorFlow の高レベルの概念を用います。\n",
    "\n",
    "* Use TensorFlow's default [eager execution](https://www.tensorflow.org/guide/eager) development environment,\n",
    "* Import data with the [Datasets API](https://www.tensorflow.org/guide/datasets),\n",
    "* Build models and layers with TensorFlow's [Keras API](https://keras.io/getting-started/sequential-model-guide/).\n",
    "\n",
    "* TensorFlow の既定の [Eager Execution](https://www.tensorflow.org/guide/eager) 開発環境を用いて、\n",
    "* データを [Datasets API](https://www.tensorflow.org/guide/datasets) を使ってインポートし、\n",
    "* TensorFlow の [Keras API](https://keras.io/getting-started/sequential-model-guide/) を使ってモデルと層を構築する\n",
    "\n",
    "This tutorial is structured like many TensorFlow programs:\n",
    "\n",
    "このチュートリアルは、多くの TensorFlow のプログラムと同様に下記のように構成されています。\n",
    "\n",
    "1. Import and parse the dataset.\n",
    "2. Select the type of model.\n",
    "3. Train the model.\n",
    "4. Evaluate the model's effectiveness.\n",
    "5. Use the trained model to make predictions.\n",
    "\n",
    "1. データセットのインポートとパース\n",
    "2. モデルのタイプの選択\n",
    "3. モデルの訓練\n",
    "4. モデルの有効性の評価\n",
    "5. 訓練済みモデルを使った予測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yNr7H-AIoLOR"
   },
   "source": [
    "## Setup program\n",
    "## プログラムの設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1J3AuPBT9gyR"
   },
   "source": [
    "### Configure imports\n",
    "### インポートの構成\n",
    "\n",
    "Import TensorFlow and the other required Python modules. By default, TensorFlow uses [Eager Execution](https://www.tensorflow.org/guide/eager) to evaluate operations immediately, returning concrete values instead of creating a [computational graph](https://www.tensorflow.org/guide/graphs) that is executed later. If you are used to a REPL or the `python` interactive console, this feels familiar.\n",
    "\n",
    "TensorFlow とその他必要な Python のモジュールをインポートします。TensorFlow は既定で [Eager Execution](https://www.tensorflow.org/guide/eager) を使って演算結果を即時に評価し、後で実行される [computational graph（計算グラフ）](https://www.tensorflow.org/guide/graphs) を構築する代わりに具体的な値を返します。REPL や `python` の対話的コンソールを使っている方にとっては、馴染みのあるものです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jElLULrDhQZR"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bfV2Dai0Ow2o"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.0.0-beta1\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g4Wzg69bnwK2"
   },
   "outputs": [],
   "source": [
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zx7wc0LuuxaJ"
   },
   "source": [
    "## The Iris classification problem\n",
    "## アヤメ分類問題\n",
    "\n",
    "Imagine you are a botanist seeking an automated way to categorize each Iris flower you find. Machine learning provides many algorithms to classify flowers statistically. For instance, a sophisticated machine learning program could classify flowers based on photographs. Our ambitions are more modest—we're going to classify Iris flowers based on the length and width measurements of their [sepals](https://en.wikipedia.org/wiki/Sepal) and [petals](https://en.wikipedia.org/wiki/Petal).\n",
    "\n",
    "あなたが植物学者で、発見したアヤメの花を分類する自動的な方法を探しているとします。機械学習には花を統計的に分類するための方法がたくさんあります。たとえば、高度な機械学習プログラムを使うと、写真を元に花を分類することができます。私達の願望はもう少し控えめです。アヤメの花を [sepals（萼片）](https://en.wikipedia.org/wiki/Sepal) と [petals（花弁）](https://en.wikipedia.org/wiki/Petal) の長さと幅を使って分類することにしましょう。\n",
    "\n",
    "The Iris genus entails about 300 species, but our program will only classify the following three:\n",
    "\n",
    "アヤメ属にはおよそ300の種がありますが、ここで扱うプログラムは次の3つの種類のみを分類します。\n",
    "\n",
    "* Iris setosa\n",
    "* Iris virginica\n",
    "* Iris versicolor\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://www.tensorflow.org/images/iris_three_species.jpg\"\n",
    "         alt=\"Petal geometry compared for three iris species: Iris setosa, Iris virginica, and Iris versicolor\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>図1.</b> <a href=\"https://commons.wikimedia.org/w/index.php?curid=170298\">Iris setosa</a> (by <a href=\"https://commons.wikimedia.org/wiki/User:Radomil\">Radomil</a>, CC BY-SA 3.0), <a href=\"https://commons.wikimedia.org/w/index.php?curid=248095\">Iris versicolor</a>, (by <a href=\"https://commons.wikimedia.org/wiki/User:Dlanglois\">Dlanglois</a>, CC BY-SA 3.0), and <a href=\"https://www.flickr.com/photos/33397993@N05/3352169862\">Iris virginica</a> (by <a href=\"https://www.flickr.com/photos/33397993@N05\">Frank Mayfield</a>, CC BY-SA 2.0).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>\n",
    "\n",
    "Fortunately, someone has already created a [dataset of 120 Iris flowers](https://en.wikipedia.org/wiki/Iris_flower_data_set) with the sepal and petal measurements. This is a classic dataset that is popular for beginner machine learning classification problems.\n",
    "\n",
    "幸いにして、萼片と花弁の計測値を使った [120のアヤメのデータセット](https://en.wikipedia.org/wiki/Iris_flower_data_set) を作ってくれた方がいます。これは初歩の機械学習による分類問題でよく使われる古典的なデータセットです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Px6KAg0Jowz"
   },
   "source": [
    "## Import and parse the training dataset\n",
    "## 訓練用データセットのインポートとパース\n",
    "\n",
    "Download the dataset file and convert it into a structure that can be used by this Python program.\n",
    "\n",
    "データセットファイルをダウンロードし、この Python プログラムで使えるような構造に変換します。\n",
    "\n",
    "### Download the dataset\n",
    "### データセットのダウンロード\n",
    "\n",
    "Download the training dataset file using the [tf.keras.utils.get_file](https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file) function. This returns the file path of the downloaded file:\n",
    "\n",
    "[tf.keras.utils.get_file](https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file) 関数を使って訓練用データセットをダウンロードします。この関数は、ダウンロードしたファイルのパスを返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J6c7uEU9rjRM"
   },
   "outputs": [],
   "source": [
    "train_dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\"\n",
    "\n",
    "train_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url),\n",
    "                                           origin=train_dataset_url)\n",
    "\n",
    "print(\"Local copy of the dataset file: {}\".format(train_dataset_fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qnX1-aLors4S"
   },
   "source": [
    "### Inspect the data\n",
    "### データの観察\n",
    "\n",
    "This dataset, `iris_training.csv`, is a plain text file that stores tabular data formatted as comma-separated values (CSV). Use the `head -n5` command to take a peek at the first five entries:\n",
    "\n",
    "このデータセット `iris_training.csv` は、表形式のデータをコンマ区切り値（CSV）形式で格納したプレインテキストファイルです。`head -n5` コマンドを使って最初の5つのエントリを見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FQvb_JYdrpPm"
   },
   "outputs": [],
   "source": [
    "!head -n5 {train_dataset_fp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kQhzD6P-uBoq"
   },
   "source": [
    "From this view of the dataset, notice the following:\n",
    "\n",
    "上記のようにデータセットを見ることで次のことがわかります。\n",
    "\n",
    "1. The first line is a header containing information about the dataset:\n",
    "  * There are 120 total examples. Each example has four features and one of three possible label names.\n",
    "2. Subsequent rows are data records, one *[example](https://developers.google.com/machine-learning/glossary/#example)* per line, where:\n",
    "  * The first four fields are *[features](https://developers.google.com/machine-learning/glossary/#feature)*: these are the characteristics of an example. Here, the fields hold float numbers representing flower measurements.\n",
    "  * The last column is the *[label](https://developers.google.com/machine-learning/glossary/#label)*: this is the value we want to predict. For this dataset, it's an integer value of 0, 1, or 2 that corresponds to a flower name.\n",
    "\n",
    "1. 最初の行はデータセットに関する情報を含むヘッダである\n",
    "  * 全体で120のサンプルがある。各サンプルには4つの特徴量があり、3つのラベルの1つが当てはまる。\n",
    "2. 続く行はデータレコードで、1行が1つの *[example（サンプル）](https://developers.google.com/machine-learning/glossary/#example)* である。ここでは、\n",
    "  * 最初の4つのフィールドはサンプルの特徴を示す *[features（特徴量）](https://developers.google.com/machine-learning/glossary/#feature)* である。ここで各フィールドは花の計測値を表す浮動小数点値をもつ。\n",
    "  * 最後の列は予想の対象となる *[label（ラベル）](https://developers.google.com/machine-learning/glossary/#label)* である。このデータセットでは、花の名前に対応する整数値、0、1、2 のいずれかである。\n",
    "\n",
    "\n",
    "Let's write that out in code:\n",
    "\n",
    "コードを書いてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Edhevw7exl6"
   },
   "outputs": [],
   "source": [
    "# column order in CSV file\n",
    "# CSV ファイルの列の順序\n",
    "column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "\n",
    "feature_names = column_names[:-1]\n",
    "label_name = column_names[-1]\n",
    "\n",
    "print(\"Features: {}\".format(feature_names))\n",
    "print(\"Label: {}\".format(label_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CCtwLoJhhDNc"
   },
   "source": [
    "Each label is associated with string name (for example, \"setosa\"), but machine learning typically relies on numeric values. The label numbers are mapped to a named representation, such as:\n",
    "\n",
    "ラベルはそれぞれ文字列の名前（たとえば、\"setosa\"）に関連付けられていますが、機械学習は数値に依存している場合がほとんどです。ラベルの数値は名前付きの表現にマップされます。たとえば次のようになります。\n",
    "\n",
    "* `0`: Iris setosa\n",
    "* `1`: Iris versicolor\n",
    "* `2`: Iris virginica\n",
    "\n",
    "For more information about features and labels, see the [ML Terminology section of the Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/framing/ml-terminology).\n",
    "\n",
    "特徴量とラベルについてもっと知りたい場合には、 [ML Terminology section of the Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/framing/ml-terminology) を参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sVNlJlUOhkoX"
   },
   "outputs": [],
   "source": [
    "class_names = ['Iris setosa', 'Iris versicolor', 'Iris virginica']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dqPkQExM2Pwt"
   },
   "source": [
    "### Create a `tf.data.Dataset`\n",
    "### `tf.data.Dataset` の作成\n",
    "\n",
    "TensorFlow's [Dataset API](https://www.tensorflow.org/guide/datasets) handles many common cases for loading data into a model. This is a high-level API for reading data and transforming it into a form used for training. See the [Datasets Quick Start guide](https://www.tensorflow.org/get_started/datasets_quickstart) for more information.\n",
    "\n",
    "TensorFlow の [Dataset API](https://www.tensorflow.org/guide/datasets) はデータのモデルへのロードについて、多くの一般的なケースを扱います。この API は、データを読み込んで訓練に使われる形態へ変換するための、高レベル API です。詳しくは、 [Datasets Quick Start guide](https://www.tensorflow.org/get_started/datasets_quickstart) を参照してください。\n",
    "\n",
    "\n",
    "Since the dataset is a CSV-formatted text file, use the [make_csv_dataset](https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset) function to parse the data into a suitable format. Since this function generates data for training models, the default behavior is to shuffle the data (`shuffle=True, shuffle_buffer_size=10000`), and repeat the dataset forever (`num_epochs=None`). We also set the [batch_size](https://developers.google.com/machine-learning/glossary/#batch_size) parameter:\n",
    "\n",
    "今回のデータセットは CSV 形式のテキストファイルなので、データをパースして適切なフォーマットに変換するため、 [make_csv_dataset](https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset) 関数を使います。この関数はモデルの訓練のためのデータを生成するので、既定の動作はデータをシャッフルし (`shuffle=True, shuffle_buffer_size=10000`) 、データセットを永遠に繰り返すこと (`num_epochs=None`) です。また、 [batch_size](https://developers.google.com/machine-learning/glossary/#batch_size) パラメータも設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WsxHnz1ebJ2S"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    train_dataset_fp,\n",
    "    batch_size,\n",
    "    column_names=column_names,\n",
    "    label_name=label_name,\n",
    "    num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gB_RSn62c-3G"
   },
   "source": [
    "The `make_csv_dataset` function returns a `tf.data.Dataset` of `(features, label)` pairs, where `features` is a dictionary: `{'feature_name': value}`\n",
    "\n",
    "`make_csv_dataset` 関数は、 `(features, label)` というペアの `tf.data.Dataset` を返します。ここで、 `features` は、 `{'feature_name': value}` というディクショナリです。\n",
    "\n",
    "These `Dataset` objects are iterable. Let's look at a batch of features:\n",
    "\n",
    "この `Dataset` オブジェクトはイテラブルです。特徴量のバッチを1つ見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iDuG94H-C122"
   },
   "outputs": [],
   "source": [
    "features, labels = next(iter(train_dataset))\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E63mArnQaAGz"
   },
   "source": [
    "Notice that like-features are grouped together, or *batched*. Each example row's fields are appended to the corresponding feature array. Change the `batch_size` to set the number of examples stored in these feature arrays.\n",
    "\n",
    "「特徴量のようなもの」はグループ化、言い換えると *バッチ化* されることに注意してください。それぞれのサンプル行のフィールドは、対応する特徴量の配列に追加されます。これらの特徴量配列に保存されるサンプルの数を設定するには、 `batch_size` を変更します。\n",
    "\n",
    "You can start to see some clusters by plotting a few features from the batch:\n",
    "\n",
    "上記のバッチから特徴量のいくつかをプロットしてみると、いくつかのクラスタが見られます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "me5Wn-9FcyyO"
   },
   "outputs": [],
   "source": [
    "plt.scatter(features['petal_length'],\n",
    "            features['sepal_length'],\n",
    "            c=labels,\n",
    "            cmap='viridis')\n",
    "\n",
    "plt.xlabel(\"Petal length\")\n",
    "plt.ylabel(\"Sepal length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YlxpSyHlhT6M"
   },
   "source": [
    "To simplify the model building step, create a function to repackage the features dictionary into a single array with shape: `(batch_size, num_features)`.\n",
    "\n",
    "モデル構築のステップを単純化するため、特徴量のディクショナリを `(batch_size, num_features)` という形状の単一の配列にパッケージし直す関数を作成します。\n",
    "\n",
    "This function uses the [tf.stack](https://www.tensorflow.org/api_docs/python/tf/stack) method which takes values from a list of tensors and creates a combined tensor at the specified dimension:\n",
    "\n",
    "この関数は、テンソルのリストから値を受け取り、指定された次元で結合されたテンソルを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jm932WINcaGU"
   },
   "outputs": [],
   "source": [
    "def pack_features_vector(features, labels):\n",
    "  \"\"\"Pack the features into a single array.\"\"\"\n",
    "  \"\"\"特徴量を1つの配列にパックする\"\"\"\n",
    "  features = tf.stack(list(features.values()), axis=1)\n",
    "  return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V1Vuph_eDl8x"
   },
   "source": [
    "Then use the [tf.data.Dataset.map](https://www.tensorflow.org/api_docs/python/tf/data/dataset/map) method to pack the `features` of each `(features,label)` pair into the training dataset:\n",
    "\n",
    "次に、 [tf.data.Dataset.map](https://www.tensorflow.org/api_docs/python/tf/data/dataset/map) メソッドを使って `(features,label)` ペアそれぞれの `features` を訓練用データセットにパックします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZbDkzGZIkpXf"
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(pack_features_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NLy0Q1xCldVO"
   },
   "source": [
    "The features element of the `Dataset` are now arrays with shape `(batch_size, num_features)`. Let's look at the first few examples:\n",
    "\n",
    "この `Dataset` の特徴量要素は、いまは形状 `(batch_size, num_features)` の配列になっています。 最初のサンプルをいくつか見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kex9ibEek6Tr"
   },
   "outputs": [],
   "source": [
    "features, labels = next(iter(train_dataset))\n",
    "\n",
    "print(features[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LsaVrtNM3Tx5"
   },
   "source": [
    "## Select the type of model\n",
    "## モデルのタイプの選択\n",
    "\n",
    "### Why model?\n",
    "### なぜモデルか？\n",
    "\n",
    "A *[model](https://developers.google.com/machine-learning/crash-course/glossary#model)* is a relationship between features and the label.  For the Iris classification problem, the model defines the relationship between the sepal and petal measurements and the predicted Iris species. Some simple models can be described with a few lines of algebra, but complex machine learning models have a large number of parameters that are difficult to summarize.\n",
    "\n",
    "*[model（モデル）](https://developers.google.com/machine-learning/crash-course/glossary#model)* は特徴量とラベルの間の関係です。アヤメの分類問題の場合、モデルは萼片と花弁の計測値と予測されるアヤメの種類の関係を定義します。単純なモデルであれば、2、3行の数式で記述できますが、複雑な機械学習モデルには多数のパラメータがあり、かんたんに要約できるものではありません。\n",
    "\n",
    "Could you determine the relationship between the four features and the Iris species *without* using machine learning?  That is, could you use traditional programming techniques (for example, a lot of conditional statements) to create a model?  Perhaps—if you analyzed the dataset long enough to determine the relationships between petal and sepal measurements to a particular species. And this becomes difficult—maybe impossible—on more complicated datasets. A good machine learning approach *determines the model for you*. If you feed enough representative examples into the right machine learning model type, the program will figure out the relationships for you.\n",
    "\n",
    "機械学習を*使わずに*これらの4つの特徴量とアヤメの品種の関係を決定することは可能でしょうか？言い換えると、従来のプログラミング技術（たとえば、たくさんの条件文）を使ってモデルを作ることは可能でしょうか？おそらく、十分に時間をかけてデータセットを分析して、萼片と花弁の計測値と特定の品種との関係を決定すれば可能でしょう。もっと複雑なデータセットの場合には、これは困難であり、おそらく不可能です。機械学習を使ったよいアプローチでは、*あなたに代わってモデルを決定*してくれます。適切なタイプの機械学習モデルに、十分な量の典型的なサンプルを投入すれば、プログラムがあなたのためにこの関係性をみつけ出してくれるでしょう。\n",
    "\n",
    "### Select the model\n",
    "###  モデルの選択\n",
    "\n",
    "We need to select the kind of model to train. There are many types of models and picking a good one takes experience. This tutorial uses a neural network to solve the Iris classification problem. *[Neural networks](https://developers.google.com/machine-learning/glossary/#neural_network)* can find complex relationships between features and the label. It is a highly-structured graph, organized into one or more *[hidden layers](https://developers.google.com/machine-learning/glossary/#hidden_layer)*. Each hidden layer consists of one or more *[neurons](https://developers.google.com/machine-learning/glossary/#neuron)*. There are several categories of neural networks and this program uses a dense, or *[fully-connected neural network](https://developers.google.com/machine-learning/glossary/#fully_connected_layer)*: the neurons in one layer receive input connections from *every* neuron in the previous layer. For example, Figure 2 illustrates a dense neural network consisting of an input layer, two hidden layers, and an output layer:\n",
    "\n",
    "訓練すべきモデルの種類を決める必要があります。モデルにはたくさんの種類があり、よいものを選択するには経験が必要です。このチュートリアルでは、アヤメの分類問題を解くために、ニューラルネットワークを使用します。*[Neural networks（ニューラルネットワーク）](https://developers.google.com/machine-learning/glossary/#neural_network)* は、特徴量とラベルの間の複雑な関係をみつけることができます。ニューラルネットワークは、1つかそれ以上の *[hidden layers（隠れ層）](https://developers.google.com/machine-learning/glossary/#hidden_layer)* に構成された高度なグラフ構造です。それぞれの隠れ層には1つかそれ以上の *[neurons（ニューロン）](https://developers.google.com/machine-learning/glossary/#neuron)* があります。ニューラルネットワークにはいくつかのカテゴリーがありますが、このプログラムでは、密な、あるいは、 *[fully-connected neural network（全結合ニューラルネットワーク）](https://developers.google.com/machine-learning/glossary/#fully_connected_layer)* を使用します。全結合ニューラルネットワークでは、1つの層の中のニューロンすべてが、前の層の*すべての*ニューロンからの入力を受け取ります。例として、図2に入力層、2つの隠れ層、そして、出力層からなる密なニューラルネットワークを図示します。\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://www.tensorflow.org/images/custom_estimators/full_network.png\"\n",
    "         alt=\"A diagram of the network architecture: Inputs, 2 hidden layers, and outputs\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>図2.</b> 特徴量と隠れ層、予測をもつニューラルネットワーク<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>\n",
    "\n",
    "When the model from Figure 2 is trained and fed an unlabeled example, it yields three predictions: the likelihood that this flower is the given Iris species. This prediction is called *[inference](https://developers.google.com/machine-learning/crash-course/glossary#inference)*. For this example, the sum of the output predictions is 1.0. In Figure 2, this prediction breaks down as: `0.02` for *Iris setosa*, `0.95` for *Iris versicolor*, and `0.03` for *Iris virginica*. This means that the model predicts—with 95% probability—that an unlabeled example flower is an *Iris versicolor*.\n",
    "\n",
    "図2のモデルが訓練されてラベルの付いていないサンプルを受け取ったとき、モデルは3つの予測値を返します。予測値はサンプルの花が与えられた3つの品種のそれぞれである可能性を示します。この予測は、 *[inference（推論）](https://developers.google.com/machine-learning/crash-course/glossary#inference)* とも呼ばれます。このサンプルでは、出力の予測値の合計は 1.0 になります。図2では、この予測値は *Iris setosa* が `0.02` 、 *Iris versicolor* が `0.95` 、 *Iris virginica* が `0.03` となっています。これは、モデルがこのラベルのない花を、95% の確率で *Iris versicolor* であると予測したことを意味します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W23DIMVPQEBt"
   },
   "source": [
    "### Create a model using Keras\n",
    "### Keras を使ったモデル構築\n",
    "\n",
    "The TensorFlow [tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras) API is the preferred way to create models and layers. This makes it easy to build models and experiment while Keras handles the complexity of connecting everything together.\n",
    "\n",
    "TensorFlow の [tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras) API は、モデルと層を作成するための好ましい方法です。Keras がすべてを結びつけるという複雑さを引き受けてくれるため、モデルや実験の構築がかんたんになります。\n",
    "\n",
    "The [tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) model is a linear stack of layers. Its constructor takes a list of layer instances, in this case, two [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layers with 10 nodes each, and an output layer with 3 nodes representing our label predictions. The first layer's `input_shape` parameter corresponds to the number of features from the dataset, and is required:\n",
    "\n",
    "[tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) モデルは層が一列に積み上げられたものです。このクラスのコンストラクタは、レイヤーインスタンスのリストを引数として受け取ります。今回の場合、それぞれ 10 のノードをもつ 2つの [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) レイヤーと、ラベルの予測値を表す3つのノードからなる出力レイヤーです。最初のレイヤーの `input_shape` パラメータがデータセットの特徴量の数に対応しており、これは必須パラメータです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2fZ6oL2ig3ZK"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  # input shape required\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(3)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FHcbEzMpxbHL"
   },
   "source": [
    "The *[activation function](https://developers.google.com/machine-learning/crash-course/glossary#activation_function)* determines the output shape of each node in the layer. These non-linearities are important—without them the model would be equivalent to a single layer. There are many [available activations](https://www.tensorflow.org/api_docs/python/tf/keras/activations), but [ReLU](https://developers.google.com/machine-learning/crash-course/glossary#ReLU) is common for hidden layers.\n",
    "\n",
    "*[activation function（活性化関数）](https://developers.google.com/machine-learning/crash-course/glossary#activation_function)*  は、そのレイヤーの各ノードの出力の形を決定します。この関数の非線形性は重要であり、それがなければモデルは 1層しかないものと等価になってしまいます。[利用可能な活性化関数](https://www.tensorflow.org/api_docs/python/tf/keras/activations) はたくさんありますが、隠れ層では [ReLU](https://developers.google.com/machine-learning/crash-course/glossary#ReLU) が一般的です。\n",
    "\n",
    "The ideal number of hidden layers and neurons depends on the problem and the dataset. Like many aspects of machine learning, picking the best shape of the neural network requires a mixture of knowledge and experimentation. As a rule of thumb, increasing the number of hidden layers and neurons typically creates a more powerful model, which requires more data to train effectively.\n",
    "\n",
    "理想的な隠れ層の数やニューロンの数は問題やデータセットによって異なります。機械学習のさまざまな面と同様に、ニューラルネットワークの最良の形を選択するには、知識と経験の両方が必要です。経験則から、一般的には隠れ層やニューロンの数を増やすとより強力なモデルを作ることができますが、効果的に訓練を行うためにより多くのデータを必要とします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2wFKnhWCpDSS"
   },
   "source": [
    "### Using the model\n",
    "### モデルの使用\n",
    "\n",
    "Let's have a quick look at what this model does to a batch of features:\n",
    "\n",
    "それでは、このモデルが特徴量のバッチに対して何を行うかを見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xe6SQ5NrpB-I"
   },
   "outputs": [],
   "source": [
    "predictions = model(features)\n",
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxyXOhwVr5S3"
   },
   "source": [
    "Here, each example returns a [logit](https://developers.google.com/machine-learning/crash-course/glossary#logits) for each class.\n",
    "\n",
    "ご覧のように、サンプルのそれぞれは、各クラスの [logit（ロジット）](https://developers.google.com/machine-learning/crash-course/glossary#logits) 値を返します。\n",
    "\n",
    "\n",
    "To convert these logits to a probability for each class, use the [softmax](https://developers.google.com/machine-learning/crash-course/glossary#softmax) function:\n",
    "\n",
    "これらのロジット値を各クラスの確率に変換するためには、 [softmax](https://developers.google.com/machine-learning/crash-course/glossary#softmax) 関数を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_tRwHZmTNTX2"
   },
   "outputs": [],
   "source": [
    "tf.nn.softmax(predictions[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uRZmchElo481"
   },
   "source": [
    "Taking the `tf.argmax` across classes gives us the predicted class index. But, the model hasn't been trained yet, so these aren't good predictions:\n",
    "\n",
    "クラス間で `tf.argmax` を取ると、予測されたクラスのインデックスが得られます。しかし、このモデルはまだ訓練されていないので、予測はよいものではありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Jzm_GoErz8B"
   },
   "outputs": [],
   "source": [
    "print(\"Prediction: {}\".format(tf.argmax(predictions, axis=1)))\n",
    "print(\"    Labels: {}\".format(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vzq2E5J2QMtw"
   },
   "source": [
    "## Train the model\n",
    "## モデルの訓練\n",
    "\n",
    "*[Training](https://developers.google.com/machine-learning/crash-course/glossary#training)* is the stage of machine learning when the model is gradually optimized, or the model *learns* the dataset. The goal is to learn enough about the structure of the training dataset to make predictions about unseen data. If you learn *too much* about the training dataset, then the predictions only work for the data it has seen and will not be generalizable. This problem is called *[overfitting](https://developers.google.com/machine-learning/crash-course/glossary#overfitting)*—it's like memorizing the answers instead of understanding how to solve a problem.\n",
    "\n",
    "*[Training（訓練）](https://developers.google.com/machine-learning/crash-course/glossary#training)* は、機械学習において、モデルが徐々に最適化されていく、あるいはモデルがデータセットを*学習する*段階です。目的は、見たことのないデータについて予測を行うため、訓練用データセットの構造を十分に学習することです。訓練用データセットを学習*しすぎる*と、予測は見たことのあるデータに対してしか有効ではなく、一般化できません。この問題は *[overfitting（過学習）](https://developers.google.com/machine-learning/crash-course/glossary#overfitting)* と呼ばれ、問題の解き方を理解するのではなく答えを丸暗記するようなものです。\n",
    "\n",
    "The Iris classification problem is an example of *[supervised machine learning](https://developers.google.com/machine-learning/glossary/#supervised_machine_learning)*: the model is trained from examples that contain labels. In *[unsupervised machine learning](https://developers.google.com/machine-learning/glossary/#unsupervised_machine_learning)*, the examples don't contain labels. Instead, the model typically finds patterns among the features.\n",
    "\n",
    "アヤメの分類問題は、 *[supervised machine learning（教師あり学習）](https://developers.google.com/machine-learning/glossary/#supervised_machine_learning)* の1種で、モデルはラベルの付いたサンプルを学習します。*[unsupervised machine learning（教師なし学習）](https://developers.google.com/machine-learning/glossary/#unsupervised_machine_learning)* では、サンプルにラベルはありません。その代わり、一般的にはモデルが特徴量からパターンを発見します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RaKp8aEjKX6B"
   },
   "source": [
    "### Define the loss and gradient function\n",
    "### 損失関数と勾配関数の定義\n",
    "\n",
    "Both training and evaluation stages need to calculate the model's *[loss](https://developers.google.com/machine-learning/crash-course/glossary#loss)*. This measures how off a model's predictions are from the desired label, in other words, how bad the model is performing. We want to minimize, or optimize, this value.\n",
    "\n",
    "訓練段階と評価段階では、モデルの *[loss（損失）](https://developers.google.com/machine-learning/crash-course/glossary#loss)* を計算する必要があります。損失とは、モデルの予測が望ましいラベルからどれくらい離れているかを測定するものです。言い換えると、どれくらいモデルの性能が悪いかを示します。この値を最小化、または最適化したいのです。\n",
    "\n",
    "Our model will calculate its loss using the `tf.keras.losses.SparseCategoricalCrossentropy` function which takes the model's class probability predictions and the desired label, and returns the average loss across the examples.\n",
    "\n",
    "今回のモデルでは、 `tf.keras.losses.SparseCategoricalCrossentropy` 関数を使って損失を計算します。これは、モデルのクラスごとの予測確率と望まれるラベルを使って、サンプル間の平均損失を返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QOsi6b-1CXIn"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tMAT4DcMPwI-"
   },
   "outputs": [],
   "source": [
    "def loss(model, x, y):\n",
    "  y_ = model(x)\n",
    "\n",
    "  return loss_object(y_true=y, y_pred=y_)\n",
    "\n",
    "\n",
    "l = loss(model, features, labels)\n",
    "print(\"Loss test: {}\".format(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3IcPqA24QM6B"
   },
   "source": [
    "Use the [tf.GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape) context to calculate the *[gradients](https://developers.google.com/machine-learning/crash-course/glossary#gradient)* used to optimize your model:\n",
    "\n",
    "[tf.GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape) コンテキストを使って、モデルを最適化する際に使われる *[gradients（勾配）](https://developers.google.com/machine-learning/crash-course/glossary#gradient)* を計算しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x57HcKWhKkei"
   },
   "outputs": [],
   "source": [
    "def grad(model, inputs, targets):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss_value = loss(model, inputs, targets)\n",
    "  return loss_value, tape.gradient(loss_value, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lOxFimtlKruu"
   },
   "source": [
    "### Create an optimizer\n",
    "### オプティマイザーの作成\n",
    "\n",
    "An *[optimizer](https://developers.google.com/machine-learning/crash-course/glossary#optimizer)* applies the computed gradients to the model's variables to minimize the `loss` function. You can think of the loss function as a curved surface (see Figure 3) and we want to find its lowest point by walking around. The gradients point in the direction of steepest ascent—so we'll travel the opposite way and move down the hill. By iteratively calculating the loss and gradient for each batch, we'll adjust the model during training. Gradually, the model will find the best combination of weights and bias to minimize loss. And the lower the loss, the better the model's predictions.\n",
    "\n",
    "*[optimizer](https://developers.google.com/machine-learning/crash-course/glossary#optimizer)* は、`loss` 関数を最小化するため、計算された勾配をモデルの変数に適用します。損失関数は、局面として考えることができ（図3 参照）、歩き回ることによって最小となる点をみつけたいのです。勾配は、一番急な上りの方向を示すため、逆方向に進んで丘を下ることになります。バッチごとに損失と勾配を計算することにより、訓練中のモデルを調節します。モデルは徐々に、損失を最小化する重みとバイアスの最適な組み合わせをみつけます。損失が小さいほど、モデルの予測がよくなります。\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://cs231n.github.io/assets/nn3/opt1.gif\" width=\"70%\"\n",
    "         alt=\"Optimization algorithms visualized over time in 3D space.\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>図3.</b> 3次元空間における最適化アルゴリズムの時系列可視化。<br/>(Source: <a href=\"http://cs231n.github.io/neural-networks-3/\">Stanford class CS231n</a>, MIT License, Image credit: <a href=\"https://twitter.com/alecrad\">Alec Radford</a>)\n",
    "  </td></tr>\n",
    "</table>\n",
    "\n",
    "TensorFlow has many [optimization algorithms](https://www.tensorflow.org/api_guides/python/train) available for training. This model uses the [tf.train.GradientDescentOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer) that implements the *[stochastic gradient descent](https://developers.google.com/machine-learning/crash-course/glossary#gradient_descent)* (SGD) algorithm. The `learning_rate` sets the step size to take for each iteration down the hill. This is a *hyperparameter* that you'll commonly adjust to achieve better results.\n",
    "\n",
    "TensorFlow には、訓練に使える [optimization algorithms（最適化アルゴリズム）](https://www.tensorflow.org/api_guides/python/train) がたくさんあります。このモデルでは、*[stochastic gradient descent（確率的勾配降下法）](https://developers.google.com/machine-learning/crash-course/glossary#gradient_descent)* (SGD) を実装した [tf.train.GradientDescentOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer) アルゴリズムを使用します。`learning_rate` （学習率）は、イテレーションごとに丘を下る際のステップのサイズを設定します。通常これはよりよい結果を得るために調整する *hyperparameter（ハイパーパラメータ）* です。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XkUd6UiZa_dF"
   },
   "source": [
    "Let's setup the optimizer:\n",
    "\n",
    "オプティマイザーを設定しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8xxi2NNGKwG_"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pJVRZ0hP52ZB"
   },
   "source": [
    "We'll use this to calculate a single optimization step:\n",
    "\n",
    "これを使って、最適化を１ステップ分計算してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rxRNTFVe56RG"
   },
   "outputs": [],
   "source": [
    "loss_value, grads = grad(model, features, labels)\n",
    "\n",
    "print(\"Step: {}, Initial Loss: {}\".format(optimizer.iterations.numpy(),\n",
    "                                          loss_value.numpy()))\n",
    "\n",
    "optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "print(\"Step: {},         Loss: {}\".format(optimizer.iterations.numpy(),\n",
    "                                          loss(model, features, labels).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Y2VSELvwAvW"
   },
   "source": [
    "### Training loop\n",
    "### 訓練ループ\n",
    "\n",
    "With all the pieces in place, the model is ready for training! A training loop feeds the dataset examples into the model to help it make better predictions. The following code block sets up these training steps:\n",
    "\n",
    "すべての部品が揃ったので、モデルの訓練ができるようになりました。訓練ループは、モデルにデータセットのサンプルを供給し、モデルがよりよい予測を行えるようにします。下記のコードブロックは、この訓練のステップを構成します。\n",
    "\n",
    "1. Iterate each *epoch*. An epoch is one pass through the dataset.\n",
    "2. Within an epoch, iterate over each example in the training `Dataset` grabbing its *features* (`x`) and *label* (`y`).\n",
    "3. Using the example's features, make a prediction and compare it with the label. Measure the inaccuracy of the prediction and use that to calculate the model's loss and gradients.\n",
    "4. Use an `optimizer` to update the model's variables.\n",
    "5. Keep track of some stats for visualization.\n",
    "6. Repeat for each epoch.\n",
    "\n",
    "1. *epoch（エポック）* をひとつずつ繰り返します。エポックとは、データセットをひととおり処理するということです。\n",
    "2. エポック内では、訓練用の `Dataset（データセット）` のサンプルひとつずつから、その *features（特徴量）* (`x`) と *label（ラベル）* (`y`) を取り出して処理します。\n",
    "3. サンプルの特徴量を使って予測を行い、ラベルと比較します。予測の不正確度を測定し、それを使ってモデルの損失と勾配を計算します。\n",
    "4. `optimizer` を使って、モデルの変数を更新します。\n",
    "5. 可視化のためにいくつかの統計量を記録します。\n",
    "6. これをエポックごとに繰り返します。\n",
    "\n",
    "The `num_epochs` variable is the number of times to loop over the dataset collection. Counter-intuitively, training a model longer does not guarantee a better model. `num_epochs` is a *[hyperparameter](https://developers.google.com/machine-learning/glossary/#hyperparameter)* that you can tune. Choosing the right number usually requires both experience and experimentation:\n",
    "\n",
    "`num_epochs` 変数は、データセットのコレクションを何回繰り返すかという数字です。直感には反しますが、モデルを長く訓練すれば必ずよいモデルが得られるというわけではありません。`num_epochs` は、*[hyperparameter（ハイパーパラメータ）](https://developers.google.com/machine-learning/glossary/#hyperparameter)* であり、チューニングできます。適切な数を選択するには、経験と実験の両方が必要です。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AIgulGRUhpto"
   },
   "outputs": [],
   "source": [
    "## Note: Rerunning this cell uses the same model variables\n",
    "## Note: このセルを再実行すると同じモデル変数が使われます\n",
    "\n",
    "# Keep results for plotting\n",
    "# 結果をグラフ化のために保存\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "num_epochs = 201\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "  epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "  # Training loop - using batches of 32\n",
    "  # 訓練ループ - 32個ずつのバッチを使用\n",
    "  for x, y in train_dataset:\n",
    "    # Optimize the model\n",
    "    # モデルの最適化\n",
    "    loss_value, grads = grad(model, x, y)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Track progress\n",
    "    # 進捗の記録\n",
    "    epoch_loss_avg(loss_value)  # Add current batch loss 現在のバッチの損失を加算\n",
    "    # Compare predicted label to actual label\n",
    "    # 予測ラベルと実際のラベルを比較\n",
    "    epoch_accuracy(y, model(x))\n",
    "\n",
    "  # End epoch\n",
    "  # エポックの終わり\n",
    "  train_loss_results.append(epoch_loss_avg.result())\n",
    "  train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "  if epoch % 50 == 0:\n",
    "    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n",
    "                                                                epoch_loss_avg.result(),\n",
    "                                                                epoch_accuracy.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2FQHVUnm_rjw"
   },
   "source": [
    "### Visualize the loss function over time\n",
    "### 時間の経過に対する損失関数の可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j3wdbmtLVTyr"
   },
   "source": [
    "While it's helpful to print out the model's training progress, it's often *more* helpful to see this progress. [TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard) is a nice visualization tool that is packaged with TensorFlow, but we can create basic charts using the `matplotlib` module.\n",
    "\n",
    "モデルの訓練の進み方をプリントするのも役立ちますが、普通はこの進捗を見るほうが*もっと*役に立ちます。[TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard) は TensorFlow に付属する優れた可視化ツールですが、\n",
    " `matplotlib` モジュールを使って基本的なグラフを描画することもできます。\n",
    " \n",
    "Interpreting these charts takes some experience, but you really want to see the *loss* go down and the *accuracy* go up:\n",
    "\n",
    "これらのグラフを解釈するにはある程度の経験が必要ですが、*損失*が低下して、*正解率*が上昇するのをみたいですよね。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "agjvNd2iUGFn"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
    "fig.suptitle('Training Metrics')\n",
    "\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
    "axes[0].plot(train_loss_results)\n",
    "\n",
    "axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
    "axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
    "axes[1].plot(train_accuracy_results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zg8GoMZhLpGH"
   },
   "source": [
    "## Evaluate the model's effectiveness\n",
    "## モデルの有効性評価\n",
    "\n",
    "Now that the model is trained, we can get some statistics on its performance.\n",
    "\n",
    "モデルの訓練が終わったら、その性能の指標を得ることができます。\n",
    "\n",
    "*Evaluating* means determining how effectively the model makes predictions. To determine the model's effectiveness at Iris classification, pass some sepal and petal measurements to the model and ask the model to predict what Iris species they represent. Then compare the model's predictions against the actual label.  For example, a model that picked the correct species on half the input examples has an *[accuracy](https://developers.google.com/machine-learning/glossary/#accuracy)* of `0.5`. Figure 4 shows a slightly more effective model, getting 4 out of 5 predictions correct at 80% accuracy:\n",
    "\n",
    "*評価*とは、モデルの予測が効果的であるかどうかを決定することを言います。アヤメの分類でモデルの有効性を見定めるには、モデルに萼片と花弁の測定値をいくつか与え、どのアヤメの品種であるかを予測させます。そして、モデルの予測と実際のラベルを比較します。たとえば、あるモデルが入力サンプルの半分で正解の品種をえらんだとすると、*[accuracy（正解率）](https://developers.google.com/machine-learning/glossary/#accuracy)* は `0.5` ということになります。図４で示すモデルはもう少し効果的であり、5つの予測中4つで正解し、正解率は 80% です。\n",
    "\n",
    "<table cellpadding=\"8\" border=\"0\">\n",
    "  <colgroup>\n",
    "    <col span=\"4\" >\n",
    "    <col span=\"1\" bgcolor=\"lightblue\">\n",
    "    <col span=\"1\" bgcolor=\"lightgreen\">\n",
    "  </colgroup>\n",
    "  <tr bgcolor=\"lightgray\">\n",
    "    <th colspan=\"4\">サンプルの特徴量</th>\n",
    "    <th colspan=\"1\">ラベル</th>\n",
    "    <th colspan=\"1\" >モデルの予測値</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5.9</td><td>3.0</td><td>4.3</td><td>1.5</td><td align=\"center\">1</td><td align=\"center\">1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6.9</td><td>3.1</td><td>5.4</td><td>2.1</td><td align=\"center\">2</td><td align=\"center\">2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5.1</td><td>3.3</td><td>1.7</td><td>0.5</td><td align=\"center\">0</td><td align=\"center\">0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6.0</td> <td>3.4</td> <td>4.5</td> <td>1.6</td> <td align=\"center\">1</td><td align=\"center\" bgcolor=\"red\">2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5.5</td><td>2.5</td><td>4.0</td><td>1.3</td><td align=\"center\">1</td><td align=\"center\">1</td>\n",
    "  </tr>\n",
    "  <tr><td align=\"center\" colspan=\"6\">\n",
    "    <b>図4.</b> 正解率 80％ のアヤメ分類器<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z-EvK7hGL0d8"
   },
   "source": [
    "### Setup the test dataset\n",
    "### テスト用データセットの設定\n",
    "\n",
    "Evaluating the model is similar to training the model. The biggest difference is the examples come from a separate *[test set](https://developers.google.com/machine-learning/crash-course/glossary#test_set)* rather than the training set. To fairly assess a model's effectiveness, the examples used to evaluate a model must be different from the examples used to train the model.\n",
    "\n",
    "モデルの評価はモデルの訓練と同様です。もっとも大きな違いは、サンプルが訓練用データセットではなく*[test set（テスト用データセット）](https://developers.google.com/machine-learning/crash-course/glossary#test_set)* からのものであるという点です。モデルの有効性を正しく評価するには、モデルの評価に使うサンプルは訓練用データセットのものとは違うものでなければなりません。\n",
    "\n",
    "The setup for the test `Dataset` is similar to the setup for training `Dataset`. Download the CSV text file and parse that values, then give it a little shuffle:\n",
    "\n",
    "テスト用 `Dataset` の設定は、訓練用 `Dataset` の設定と同様です。CSV ファイルをダウンロードし、値をパースしてからすこしシャッフルを行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ps3_9dJ3Lodk"
   },
   "outputs": [],
   "source": [
    "test_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "test_fp = tf.keras.utils.get_file(fname=os.path.basename(test_url),\n",
    "                                  origin=test_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SRMWCu30bnxH"
   },
   "outputs": [],
   "source": [
    "test_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    test_fp,\n",
    "    batch_size,\n",
    "    column_names=column_names,\n",
    "    label_name='species',\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "\n",
    "test_dataset = test_dataset.map(pack_features_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HFuOKXJdMAdm"
   },
   "source": [
    "### Evaluate the model on the test dataset\n",
    "### テスト用データセットでのモデルの評価\n",
    "\n",
    "Unlike the training stage, the model only evaluates a single [epoch](https://developers.google.com/machine-learning/glossary/#epoch) of the test data. In the following code cell, we iterate over each example in the test set and compare the model's prediction against the actual label. This is used to measure the model's accuracy across the entire test set:\n",
    "\n",
    "訓練段階とは異なり、モデルはテスト用データの 1[epoch](https://developers.google.com/machine-learning/glossary/#epoch) だけで行います。下記のコードセルでは、テスト用データセットのサンプルをひとつずつ処理し、モデルの予測と実際のラベルを比較します。この結果を使ってテスト用データセット全体でのモデルの正解率を計算します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tw03-MK1cYId"
   },
   "outputs": [],
   "source": [
    "test_accuracy = tf.keras.metrics.Accuracy()\n",
    "\n",
    "for (x, y) in test_dataset:\n",
    "  logits = model(x)\n",
    "  prediction = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "  test_accuracy(prediction, y)\n",
    "\n",
    "print(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcKEZMtCOeK-"
   },
   "source": [
    "We can see on the last batch, for example, the model is usually correct:\n",
    "\n",
    "たとえば最後のバッチを見ると、モデルはだいたい正確です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uNwt2eMeOane"
   },
   "outputs": [],
   "source": [
    "tf.stack([y,prediction],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Li2r1tYvW7S"
   },
   "source": [
    "## Use the trained model to make predictions\n",
    "## 訓練済みモデルを使った予測\n",
    "\n",
    "We've trained a model and \"proven\" that it's good—but not perfect—at classifying Iris species. Now let's use the trained model to make some predictions on [unlabeled examples](https://developers.google.com/machine-learning/glossary/#unlabeled_example); that is, on examples that contain features but not a label.\n",
    "\n",
    "モデルを訓練し、それが、完全ではないにせよアヤメの品種の分類でよい性能を示すことを「証明」しました。次は、この訓練済みモデルを使って [unlabeled examples（ラベルなしサンプル）](https://developers.google.com/machine-learning/glossary/#unlabeled_example) つまり、特徴量だけでラベルのないサンプルについて予測を行ってみましょう。\n",
    "\n",
    "In real-life, the unlabeled examples could come from lots of different sources including apps, CSV files, and data feeds. For now, we're going to manually provide three unlabeled examples to predict their labels. Recall, the label numbers are mapped to a named representation as:\n",
    "\n",
    "現実世界では、ラベルなしサンプルはアプリや CSV ファイル、データフィードなどさまざまな異なるソースからやってきます。ここでは、ラベルを予測するため、3つのラベルなしサンプルを手動で与えることにします。ラベルの番号は、下記のように名前を表していることを思い出してください。\n",
    "\n",
    "* `0`: Iris setosa\n",
    "* `1`: Iris versicolor\n",
    "* `2`: Iris virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kesTS5Lzv-M2"
   },
   "outputs": [],
   "source": [
    "predict_dataset = tf.convert_to_tensor([\n",
    "    [5.1, 3.3, 1.7, 0.5,],\n",
    "    [5.9, 3.0, 4.2, 1.5,],\n",
    "    [6.9, 3.1, 5.4, 2.1]\n",
    "])\n",
    "\n",
    "predictions = model(predict_dataset)\n",
    "\n",
    "for i, logits in enumerate(predictions):\n",
    "  class_idx = tf.argmax(logits).numpy()\n",
    "  p = tf.nn.softmax(logits)[class_idx]\n",
    "  name = class_names[class_idx]\n",
    "  print(\"Example {} prediction: {} ({:4.1f}%)\".format(i, name, 100*p))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "custom_training_walkthrough",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
